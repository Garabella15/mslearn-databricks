# Exercise 05 - Evaluate Large Language Models using Azure Databricks and Azure OpenAI

## Objective
In this exercise, you will learn how to evaluate large language models (LLMs) using Azure Databricks and the GPT-4 OpenAI model. This includes setting up the environment, defining evaluation metrics, and analyzing the model's performance on specific tasks.

## Requirements
An active Azure subscription. If you do not have one, you can sign up for a [free trial](https://azure.microsoft.com/en-us/free/).

## Step 1: Provision Azure Databricks
- Login to Azure Portal:
    1. Go to Azure Portal and sign in with your credentials.
- Create Databricks Service:
    1. Navigate to "Create a resource" > "Analytics" > "Azure Databricks".
    2. Enter the necessary details like workspace name, subscription, resource group (create new or select existing), and location.
    3. Select the pricing tier (choose standard for this lab).
    4. Click "Review + create" and then "Create" once validation passes.

## Step 2: Launch Workspace and Create a Cluster
- Launch Databricks Workspace:
    1. Once the deployment is complete, go to the resource and click "Launch Workspace".
- Create a Spark Cluster:
    1. In the Databricks workspace, click "Compute" on the sidebar, then "Create compute".
    2. Specify the cluster name and select a runtime version of Spark.
    3. Choose the Worker type as "Standard" and node type based on available options (choose smaller nodes for cost-efficiency).
    4. Click "Create compute".

## Step 3: Install required Libraries

- Log in to your Azure Databricks workspace.
- Create a new notebook and select the default cluster.
- Run the following commands to install the necessary Python libraries:

```python
%pip install openai
%pip install transformers
%pip install datasets
```

- Configure OpenAI API Key:
    1. Add your Azure OpenAI API key to the notebook:

    ```python
    import openai
    openai.api_key = "your-openai-api-key"
    ```

## Step 4: Define Evaluation Metrics
- Define Common Evaluation Metrics:
    1. In this step, you will define evaluation metrics such as Perplexity, BLEU score, ROUGE score, and accuracy depending on the task.

    ```python
    from datasets import load_metric

    # Example: Load BLEU metric
    bleu_metric = load_metric("bleu")
    rouge_metric = load_metric("rouge")

    def compute_bleu(predictions, references):
        return bleu_metric.compute(predictions=predictions, references=references)

    def compute_rouge(predictions, references):
        return rouge_metric.compute(predictions=predictions, references=references)
    ```

- Define Task-specific Metrics:
    1. Depending on the use case, define other relevant metrics. For example, for sentiment analysis, define accuracy:

    ```python
    from sklearn.metrics import accuracy_score

    def compute_accuracy(predictions, references):
        return accuracy_score(references, predictions)
    ```

## Step 5: Prepare the Dataset
- Load a Dataset
    1. Use the datasets library to load a pre-defined dataset. For this lab, you can use a simple dataset like the IMDB movie reviews dataset for sentiment analysis:

    ```python
    from datasets import load_dataset

    dataset = load_dataset("imdb")
    test_data = dataset["test"]
    ```

- Preprocess the Data
    1. Tokenize and preprocess the dataset to be compatible with the GPT-4 model:

    ```python
    from transformers import GPT2Tokenizer

    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

    def preprocess_function(examples):
        return tokenizer(examples["text"], truncation=True, padding=True)

    tokenized_data = test_data.map(preprocess_function, batched=True)
    ```

## Step 6: Evaluate the GPT-4 Model
- Generate Predictions:
    1. Use the GPT-4 model to generate predictions on the test dataset

    ```python
    def generate_predictions(input_texts):
    predictions = []
    for text in input_texts:
        response = openai.Completion.create(
            model="gpt-4",
            prompt=text,
            max_tokens=50
        )
        predictions.append(response.choices[0].text.strip())
    return predictions

    input_texts = tokenized_data["text"]
    predictions = generate_predictions(input_texts)
    ```

- Compute Evaluation Metrics
    1. Compute the evaluation metrics based on the predictions generated by the GPT-4 model

    ```python
    # Example: Compute BLEU and ROUGE scores
    bleu_score = compute_bleu(predictions, tokenized_data["text"])
    rouge_score = compute_rouge(predictions, tokenized_data["text"])

    print("BLEU Score:", bleu_score)
    print("ROUGE Score:", rouge_score)
    ```

    2. If you are evaluating for a specific task like sentiment analysis, compute the accuracy

    ```python
    # Assuming binary sentiment labels (positive/negative)
    actual_labels = test_data["label"]
    predicted_labels = [1 if "positive" in pred else 0 for pred in predictions]

    accuracy = compute_accuracy(predicted_labels, actual_labels)
    print("Accuracy:", accuracy)
    ```

## Step 7: Analyze and Interpret Results

- Interpret the Results
    1. Analyze the BLEU, ROUGE, or accuracy scores to determine how well the GPT-4 model is performing on your task.
    2. Discuss potential reasons for any discrepancies and consider ways to improve the model’s performance (e.g., fine-tuning, more data preprocessing).

- Visualize the Results
    1. Optionally, you can visualize the results using Matplotlib or any other visualization tool.

    ```python
    import matplotlib.pyplot as plt

    # Example: Plot accuracy scores
    plt.bar(["Accuracy"], [accuracy])
    plt.ylabel("Score")
    plt.title("Model Evaluation Metrics")
    plt.show()
    ```

## Step 8: Experiment with Different Scenarios

- Experiment with Different Prompts
    1. Modify the prompt structure to see how it affects the model’s performance.

- Evaluate on Different Datasets
    1. Try using a different dataset to evaluate the GPT-4 model's versatility across various tasks.

- Optimize Evaluation Metrics
    1. Experiment with hyperparameters like temperature, max tokens, etc., to optimize the evaluation metrics.

## Step 9: Clean Up Resources
- Terminate the Cluster:
    1. Go back to the "Compute" page, select your cluster, and click "Terminate" to stop the cluster.

- Optional: Delete the Databricks Service:
    1. To avoid incurring further charges, consider deleting the Databricks workspace if this lab is not part of a larger project or learning path.

This exercise guides you through the process of evaluating a large language model using Azure Databricks and the GPT-4 OpenAI model. By completing this exercise, you will gain insights into the model's performance and understand how to improve and fine-tune the model for specific tasks.